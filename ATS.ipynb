{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88762158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: docx2txt in c:\\users\\caleb\\appdata\\roaming\\python\\python39\\site-packages (0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "441fca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb80348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caleb J. Picker\n",
      "\n",
      "\tFernley, NV (available to relocate or work remotely) | (725)-217-8654 | caleb.j.picker@gmail.com |\n",
      "\n",
      "\thttps://www.linkedin.com/in/calebjpicker/ | https://github.com/calebjpicker | https://calebjpicker.quarto.pub/cvr/\n",
      "\n",
      "\n",
      "\n",
      "Qualifications\n",
      "\n",
      "Programming: R/Rstudio, Python, BigQuery. SQL, Tableau, Power BI, Excel, VBA, Excel Solver, SPSS, Mplus, C++, YAML, Quarto, Airflow, and Microsoft Office Suite\n",
      "\n",
      "Certifications/Course Work: T-SQL badge (LinkedIn), 1 R course and 3 SQL courses (Data Camp), Machine Learning (Coursera), and Mathematics for Machine Learning (Coursera)\n",
      "\n",
      "APA Workshops: Big Data: Exploratory Data Mining in Behavioral Research and Structural Equation Modeling in Longitudinal Research\n",
      "\n",
      "\tWork Experience\n",
      "\n",
      "\t\n",
      "\n",
      "\tProduction Associate, Tesla, Sparks, NV\n",
      "\n",
      "\tDec 2022 to Present\n",
      "\n",
      "Received a raise within first six months, demonstrating my exceptional work ethic, commitment to high quality, and positive collaboration\n",
      "\n",
      "Tracked and visualized versatility at workstations, providing monthly progress reports to supervisors, showcasing my dedication to performance improvement and effective communication\n",
      "\n",
      "\tMarketing Analyst, Caesars Entertainment, Las Vegas, NV\n",
      "\n",
      "\tNov 2019 to Jul 2021\n",
      "\n",
      "Created automated SQL and Python processes for P&L statements and KPI charts, significantly reducing productivity expenses by an estimated $50k per year\n",
      "\n",
      "Utilized Tableau to create a detailed zip code analysis and population density dashboard, facilitating precise market segmentation, resulting in a 5% increase in campaign performance metrics\n",
      "\n",
      "Implemented confidence intervals and test/control concepts, leading to 3% more profitable marketing campaigns and a 10% reduction in campaign costs\n",
      "\n",
      "\tPricing Analyst, American Homes 4 Rent, Las Vegas, NV\n",
      "\n",
      "\tJul 2018 to Nov 2019\n",
      "\n",
      "Initiated and led a new program to identify high-risk month-to-month renters, collaborating cross-functionally with Business Intelligence to develop Excel and Tableau dashboards for tracking progress\n",
      "\n",
      "Analyzed price elasticity of demand metrics by market over time, driving a year-over-year revenue increase of 3%\n",
      "\n",
      "Developed and executed data-driven pricing strategies based on comprehensive analysis of 22 rental markets, maintaining consistent same-home occupancy rate of 95% every quarter, compared to 93% market average\n",
      "\n",
      "\tScientist/Statistical Consultant, UNLV, Las Vegas, NV\n",
      "\n",
      "\tMay 2015 to May 2018\n",
      "\n",
      "Evaluated survey responses of introductory students, revealing psychometric soundness and linking perceived skill development to grades and GPA\n",
      "\n",
      "Automated and conducted psychometric evaluations of various scoring methods for emotional awareness, the results of which were published in a peer-reviewed journal\n",
      "\n",
      "Introduced a FORTRAN program to compare correlation differences using 95% confidence intervals, leading to a publication in a peer-reviewed journal\n",
      "\n",
      "\tMaster of Arts/Research Lab Manager, UNLV, Las Vegas, NV\n",
      "\n",
      "\tMay 2010 to May 2015\n",
      "\n",
      "Implemented project management processes, used signal detection theory, and achieved on-time, within-budget completion of multi-year projects with 99% success rate\n",
      "\n",
      "Presented 10 posters and presentations at conferences, showcasing ability to tell a narrative and draw conclusions about data in easy-to-understand narratives\n",
      "\n",
      "Mentored and managed teams of research assistants each semester, implementing standardized protocols and frameworks to reduce noisy data by 40% and maximize statistical power for detecting effects\n",
      "\n",
      "\tAcademic and Personal Projects\n",
      "\n",
      "Wrote blog on natural language processing to understand latent themes behind AFI’s music lyrics\n",
      "\n",
      "Evaluated Qualtrics survey and conducted exploratory factor analysis to improved board game group attendance and satisfaction\n",
      "\n",
      "\tEducation\n",
      "\n",
      "\tUniversity of Nevada, Las Vegas\n",
      "\n",
      "\t\n",
      "\n",
      "MA in Cognitive and Quantitative Psychology\n",
      "\n",
      "Aug 2010 to May 2015\n",
      "\n",
      "BA in Psychology\n",
      "\n",
      "Aug 2006 to May 2010\n"
     ]
    }
   ],
   "source": [
    "# Load the applicant and job descriptions\n",
    "# Set filenames to resume and jd variables\n",
    "resumedocx = \"Caleb Picker Resume draft 16 July 09 2023.docx\"\n",
    "\n",
    "# Load the files\n",
    "resume = docx2txt.process(resumedocx)\n",
    "print(resume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6e14826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roles\n",
      "\n",
      "The Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. We are looking for someone with strong hands on experience in all layers of data Integration and analytics! We especially need experience in using Python as an ETL tool. The Data Engineer plays a significant role in Agile planning, providing advice and guidance, and monitoring emerging technologies. Some of the technology we use: * Python * Informatica * SQL Server and MySQL * Vertica * Kafka\n",
      "\n",
      "Responsibilities\n",
      "\n",
      "* Strong experience with relational databases like SQL Server, MySQL and Vertica. NoSQL databases experience is a plus! \n",
      "\n",
      "* Strong background in data modeling, data access, and data storage techniques \n",
      "\n",
      "* Experience with design, development, and implementation of highly scalable, high-volume software systems and components, source of truth systems for different business areas, developing and maintaining web services in an agile environment \n",
      "\n",
      "* Working experience with Kafka Streaming layer \n",
      "\n",
      "* Experience in Spark Framework on both batch and real-time data processing is a plus \n",
      "\n",
      "* Experience in Big Data Integration & Analytics is a plus \n",
      "\n",
      "* Experience in Supply Chain and Logistics data is a plus\n",
      "\n",
      "Requirements\n",
      "\n",
      "* Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result \n",
      "\n",
      "* Derive an overall strategy of data management, within an established information architecture that supports the development and secure operation of existing and new information and digital services \n",
      "\n",
      "* Plan effective data storage, security, sharing and publishing within the organization \n",
      "\n",
      "* Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks \n",
      "\n",
      "* Ensures data quality and implements tools and frameworks for automating the identification of data quality issues \n",
      "\n",
      "* Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings \n",
      "\n",
      "* Mentor and lead data engineers providing technical guidance and oversight \n",
      "\n",
      "* Provides ongoing support, monitoring, and maintenance of deployed products\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Senior Data Scientist / Data Engineer, Drive Systems\n",
      "\n",
      "Location:Sparks, US\n",
      "\n",
      "Job Type: Full-time\n",
      "\n",
      "Job ID: 185947\n",
      "\n",
      "REFERAPPLY\n",
      "\n",
      "Roles\n",
      "\n",
      "The Drive Systems team is looking for an exceptionally talented and self-directed data scientist/data engineer to manage a large scope of data improvement projects across our production processes, validation tests, and field reliability failures. This role requires a well-rounded individual who can build, scale, and maintain data pipelines and warehousing systems and also analyze data to evaluate process health, sleuth problem areas, and drive and quantify improvements. The role will include creating and improving robust global databases and data quality standards, as well as developing scalable evaluation models and reporting systems. A strong candidate will be able to work with existing factory data and process engineering teams to build data solutions as well as drive focused investigations and improvement projects based on analysis of the data.\n",
      "\n",
      "Responsibilities\n",
      "\n",
      "·       Define data standards and storage requirements and collaborate with IT to design efficiently structured storage solutions across a range of applications and users\n",
      "\n",
      "·       Work with factory operations and field reliability teams to identify useful data sets and quantitative metrics across a range of processes and use cases and build visualizations and investigation tools\n",
      "\n",
      "·       Develop and improve automated reporting and monitoring systems for key performance metrics and statistical process control\n",
      "\n",
      "·       Perform exploratory analysis, correlation studies, design experiments, and analyze results to drive investigations and improvement projects\n",
      "\n",
      "·       Define data formatting and fidelity requirements with suppliers and internal users, build data pipelines for structured and unstructured quality and process data, and move/transform data into structured database formats in automated pipelines to enable real-time analysis\n",
      "\n",
      "Requirements\n",
      "\n",
      "·       Bachelor’s degree or higher in quantitative discipline (Statistics, Data Analytics, Computer Science, Applied Mathematics, Physics, Engineering) or the equivalent in experience and evidence of exceptional ability\n",
      "\n",
      "·       Minimum 4 years relevant working experience in analytical or quantitative roles\n",
      "\n",
      "·       Proficiency in programming languages such as Python, R, SQL, C#, JavaScript, Golang\n",
      "\n",
      "·       Proficiency in data visualization methods and tools such as Tableau, R Shiny, Dash, Plotly, etc.\n",
      "\n",
      "·       Proficiency in software deployment/management tools such as Docker, Kubernetes, Kafka, Jenkins, GitHub\n",
      "\n",
      "·       Strong working knowledge of relational and/or non-relational databases\n",
      "\n",
      "·       Significant experience with real-world automation, high volume manufacturing, and process control\n",
      "\n",
      "·       Strong working knowledge of physics and engineering principles, mathematics, and statistics\n",
      "\n",
      "·       Exceptional organization and self-direction, collaboration skills, and ability to drive efficient execution across multiple projects and priorities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sr Data Engineer\n",
      "\n",
      "Location:Sparks, US\n",
      "\n",
      "Job Type: Full-time\n",
      "\n",
      "Job ID: 137977\n",
      "\n",
      "REFERAPPLY\n",
      "\n",
      "Roles\n",
      "\n",
      "The Sr Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. We are looking for someone with strong hands on experience in all layers of data Integration and analytics! We especially need experience in using Python as an ETL tool. The Data Engineer plays a significant role in Agile planning, providing advice and guidance, and monitoring emerging technologies. Some of the technology we use: * Python * Informatica * SQL Server and MySQL * Vertica * Kafka\n",
      "\n",
      "Responsibilities\n",
      "\n",
      "* Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result \n",
      "\n",
      "* Derive an overall strategy of data management, within an established information architecture that supports the development and secure operation of existing and new information and digital services \n",
      "\n",
      "* Plan effective data storage, security, sharing and publishing within the organization \n",
      "\n",
      "* Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks \n",
      "\n",
      "* Ensures data quality and implements tools and frameworks for automating the identification of data quality issues \n",
      "\n",
      "* Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings \n",
      "\n",
      "* Mentor and lead data engineers providing technical guidance and oversight \n",
      "\n",
      "* Provides ongoing support, monitoring, and maintenance of deployed products\n",
      "\n",
      "Requirements\n",
      "\n",
      "Qualifications: \n",
      "\n",
      "* Strong experience with relational databases like SQL Server, MySQL and Vertica. NoSQL databases experience is a plus! \n",
      "\n",
      "* Strong background in data modeling, data access, and data storage techniques \n",
      "\n",
      "* Experience with design, development, and implementation of highly scalable, high-volume software systems and components, source of truth systems for different business areas, developing and maintaining web services in an agile environment \n",
      "\n",
      "* Working experience with Kafka Streaming layer \n",
      "\n",
      "* Experience in Spark Framework on both batch and real-time data processing is a plus \n",
      "\n",
      "* Experience in Big Data Integration & Analytics is a plus \n",
      "\n",
      "* Experience in Supply Chain and Logistics data is a plus\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sr. Data Engineer / Quality Engineering\n",
      "\n",
      "Location:Sparks, US\n",
      "\n",
      "Job Type: Full-time\n",
      "\n",
      "Job ID: 139360\n",
      "\n",
      "REFERAPPLY\n",
      "\n",
      "Roles\n",
      "\n",
      "Tesla's mission is to accelerate the world's transition to sustainable energy. We are committed to hiring the world's best and brightest people to help make this future a reality.\n",
      "\n",
      "Responsibilities\n",
      "\n",
      "Automating analyses and authoring pipelines using SQL, Python, Airflow, Kubernetes based ETL framework\n",
      "\n",
      "Maintaining existing data visualizations, data pipelines and dashboard enhancement requests\n",
      "\n",
      "Work effectively with engineers and conducting end-to-end analyses, from data requirement gathering, to data processing and visualization using Tableau\n",
      "\n",
      "Acquire data from primary or secondary data sources and maintain databases/data systems to empower operational and exploratory analysis\n",
      "\n",
      "Monitoring key product metrics, understanding root causes of changes in metrics\n",
      "\n",
      "Identify, analyze, and interpret trends or patterns in complex data sets and depict the story via dashboards and reports\n",
      "\n",
      "Perform data quality validations to ensure data creation is as per the business needs and expectations\n",
      "\n",
      "Work with management to prioritize business and information needs\n",
      "\n",
      "Follow best practices for scalability and performance, ensuring data quality, and maintaining documentation.\n",
      "\n",
      "Collaborate with other team members for effective knowledge transfers.\n",
      "\n",
      "Requirements\n",
      "\n",
      "Minimum of 2-3 years of experience in a data engineering/software related capacity\n",
      "\n",
      "Proficient at ad-hoc analysis using SQL queries and python data analysis packages (e.g. Pandas, Numpy)\n",
      "\n",
      "Understanding of relational database theory and proficiency in writing, understanding, and optimizing complex SQL code\n",
      "\n",
      "Experience with Business Intelligence tools (e.g. Tableau) and real time dashboard development\n",
      "\n",
      "Software engineering fundamentals, knowledge of data structure and algorithms\n",
      "\n",
      "Strong communication, organizational, and analytical and problem-solving skills\n",
      "\n",
      "Preferred: BS/MS in Management Information Systems, Computer Science, Engineering, Statistics, or another technical field\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Analyst / Quality Engineering\n",
      "\n",
      "Location:Sparks, US\n",
      "\n",
      "Job Type: Full-time\n",
      "\n",
      "Job ID: 193624\n",
      "\n",
      "REFERAPPLY\n",
      "\n",
      "Roles\n",
      "\n",
      "Tesla's mission is to accelerate the world's transition to sustainable energy. We are committed to hiring the world's best and brightest people to help make this future a reality. Every Tesla is designed to be the safest, quickest car in its class—with industry-leading safety, range and performance. Quality Engineering team plays a key role in ensuring safety of our customers by providing world class Quality of our products. Quality Data Systems team is looking for a key player on the team who can help drive Quality data analytics and help cross-functional engineering organizations to provide opportunities in product quality improvements. Candidate should have experience working with large data sets, finding best ways to engineer the data to help create critical KPI metrics, building innovative visualizations and dashboards all the while keeping in mind what improvements can be driven in underlying data systems.\n",
      "\n",
      "Responsibilities\n",
      "\n",
      "Analyze manufacturing, equipment and vehicle data and extract useful statistics and insights about failures in order to drive meaningful improvements to production quality and customer experience\n",
      "\n",
      "Work effectively with engineers and conducting end-to-end analyses, from data requirement gathering, to data processing and modeling\n",
      "\n",
      "Interpret data, analyze results using statistical techniques and provide ongoing reports\n",
      "\n",
      "Identify data sources where the potential value is not fully realized and invent new means with which to interact and gather insights from them.\n",
      "\n",
      "Monitoring key product metrics, understanding root causes of changes in metrics\n",
      "\n",
      "Identify, analyze, and interpret trends or patterns in complex data sets and depict the story via dashboards and reports\n",
      "\n",
      "Acquire data from primary or secondary data sources and maintain databases/data systems to empower operational and exploratory analysis\n",
      "\n",
      "Maintaining existing data visualizations, data pipelines and dashboard enhancement requests\n",
      "\n",
      "Automating analyses and authoring pipelines using SQL, Python, Airflow, Kubernetes based ETL framework\n",
      "\n",
      "Develop and maintain real time data streaming and processing pipeline using Kafka\n",
      "\n",
      "Drive underlying data systems improvement by working with key cross-functional stakeholders\n",
      "\n",
      "Perform data quality validations to ensure data creation is as per the business needs and expectations\n",
      "\n",
      "Work with management to prioritize business and information needs\n",
      "\n",
      "Requirements\n",
      "\n",
      "BS/MS in Management Information Systems, Computer Science, Math, Physics, Engineering, Statistics or other technical field\n",
      "\n",
      "3+ years of work experience in data analytics or engineering related field\n",
      "\n",
      "Strong knowledge of SQL and experience with multiple data architecture paradigms (MySQL, MicrosoftSQL, Vertica, Oracle, kafka, Spark)\n",
      "\n",
      "Proficient at ad-hoc analysis using SQL queries, python data analysis packages (e.g.Pandas, Numpy) , report writing and presenting findings\n",
      "\n",
      "Proficient in data visualization techniques and tools using Tableau, Power BI, Superset, Matplotlib, Plotly etc.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Analyst, Cell Materials Manufacturing\n",
      "\n",
      "Location:Sparks, US\n",
      "\n",
      "Job Type: Full-time\n",
      "\n",
      "Job ID: 187594\n",
      "\n",
      "REFERAPPLY\n",
      "\n",
      "Roles\n",
      "\n",
      "Tesla is looking for an exceptional Data Analyst to impact the Cell Recycling team. As a Data Analyst, you will be responsible for expanding and optimizing our data processes, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The right candidate will be excited by the prospect of optimizing or even re-designing our team’s data architecture to support our ever-growing recycling capacity and material flows.\n",
      "\n",
      "Responsibilities\n",
      "\n",
      "Perform statistical analysis to forecast incoming recycling volumes and identify emerging trends.\n",
      "\n",
      "Create data visualizations to communicate analysis results and drive decision making.\n",
      "\n",
      "Utilize vehicle fleet data for diagnostics and prognostics.\n",
      "\n",
      "Create and maintain optimal data pipeline architecture. Contribute to the continued automation and standardization of our data ETL.\n",
      "\n",
      "Identify, design, and implement internal process improvements: automating manual processes, and optimizing queries.\n",
      "\n",
      "Work with stakeholders including the IT Business Intelligence team to assist with data-related technical issues and support data infrastructure needs.\n",
      "\n",
      "Requirements\n",
      "\n",
      "3-5+ years of professional relevant experience\n",
      "\n",
      "BS in Computer Science, Statistics, or proof of exceptional skills in related fields.\n",
      "\n",
      "Construct and manage data workflows and aggregations to support reporting on live data connections. \n",
      "\n",
      "Strong SQL, comfortable writing queries, data processing scripts, and understanding RDBMS data structures.\n",
      "\n",
      "Strong Python, especially in a data analytics/science capacity\n",
      "\n",
      "Strong analytical skills and knowledge of applied statistics including predictive analytics, time series analysis and machine learning.\n",
      "\n",
      "Able to visualize data effectively.\n",
      "\n",
      "Tableau (or, similar BI tools).\n",
      "\n",
      "Ability to quickly learn new technologies.\n",
      "\n",
      "Nice to Have:\n",
      "\n",
      "Exposure to \"big data\" technologies, particularly any part of a Spark, HBASE, Hadoop stack.\n",
      "\n",
      "Working knowledge of reliability statistics such as Weibull Analysis.\n",
      "\n",
      "Experience using the JIRA ecosystem.\n"
     ]
    }
   ],
   "source": [
    "jddocx = \"Roles.docx\"\n",
    "jd = docx2txt.process(jddocx)\n",
    "print(jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fac24974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of text to store resume and job desription\n",
    "text = [resume,jd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "286c0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "count_matrix = cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24328bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.67508973]\n",
      " [0.67508973 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(count_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "894c601e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.51\n"
     ]
    }
   ],
   "source": [
    "match = cosine_similarity(count_matrix)[0][1]\n",
    "match = match*100\n",
    "match = round(match,2)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5a8b38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appplication is accepted\n"
     ]
    }
   ],
   "source": [
    "if(match > 60):\n",
    "    print(\"appplication is accepted\")\n",
    "else: \n",
    "    print(\"application isrejected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b768796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
